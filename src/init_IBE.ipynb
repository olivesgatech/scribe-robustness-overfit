{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"init_IBE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"POHsPxDrDZuv","colab_type":"text"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"QdKOVOgdB1vb","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P27R0fr5DfYA","colab_type":"text"},"source":["# Instantiate and make changes to model"]},{"cell_type":"code","metadata":{"id":"124jyfc0DkYM","colab_type":"code","colab":{}},"source":["net_IBE = models.resnet18(pretrained=False)\n","\n","# Replacing the first 7x7 conv stride of 4 with a 3x3 convolution kernel with \n","# stride of 1 and replacing maxpool with upsample to keep spatial features from \n","# being downsampled too quickly\n","net_IBE.conv1 = nn.Conv2d(3,64,3,stride=1,padding=1)\n","net_IBE_ = list(net_IBE.children())[:-2]\n","net_IBE_[3] = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","# The linear layer is replaced with a \"pixelwise linear layer\"\n","classifier = nn.Conv2d(512,10,1)\n","torch.nn.init.kaiming_normal_(classifier.weight)\n","net_IBE_.append(classifier)\n","\n","# No spatial aggregation; left with 10x32x32 tensor, can be used the same way as \n","# a semantic segmentation output\n","net_IBE_.append(nn.Upsample(size=32, mode='bilinear', align_corners=False))\n","net_IBE = nn.Sequential(*net_IBE_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vo3ssWzDDlB-","colab_type":"text"},"source":["# Define attention"]},{"cell_type":"code","metadata":{"id":"ltWJjOCiDn7P","colab_type":"code","colab":{}},"source":["# Logsumexp is a smooth maximum;\n","# selecting the largest component of the logit vector and then running it through \n","# sigmoid, get a value with a support from 0 to 1\n","def attention(x):\n","    return torch.sigmoid(torch.logsumexp(x, 1, keepdim=True))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fb8y0rGDHA9e","colab_type":"text"},"source":["# Set up criterion, optimizer, lr_scheduler"]},{"cell_type":"code","metadata":{"id":"XdpwWu9PHGyL","colab_type":"code","colab":{}},"source":["# The model learns a binary classifier for each class; \n","# if any class is detected then the attention is closer to 1, if not, closer to 0\n","criterion_IBE = nn.BCEWithLogitsLoss()\n","optimizer_IBE = optim.SGD(net_IBE.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n","lr_scheduler_IBE = optim.lr_scheduler.MultiStepLR(optimizer_IBE, milestones=[20, 25], gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbFnGShx2w_Z","colab_type":"code","colab":{}},"source":["# Save initial state of model\n","state = {\n","    'net': net_IBE.state_dict(),\n","    'optimizer': optimizer_IBE.state_dict(),\n","    'lr_scheduler': lr_scheduler_IBE.state_dict(),\n","}\n","torch.save(state, 'initial_state_IBE.pth')"],"execution_count":0,"outputs":[]}]}