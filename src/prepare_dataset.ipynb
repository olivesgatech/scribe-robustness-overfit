{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prepare_dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dQd9MtZ_DDOX","colab_type":"text"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"BywiBjpUw0fL","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JRMc5_6C5Y9","colab_type":"text"},"source":["# Define data augmentation transforms"]},{"cell_type":"code","metadata":{"id":"boh7XxHMC_oq","colab_type":"code","colab":{}},"source":["transform_no_aug = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","transform_first_aug = transforms.Compose([\n","    transforms.RandomCrop(32, padding=8),\n","    transform_no_aug,\n","])\n","transform_second_aug = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transform_no_aug,\n","])\n","transform_third_aug = transforms.Compose([\n","    transforms.RandomRotation(15),\n","    transform_no_aug,\n","])\n","transform_fourth_aug = transforms.Compose([\n","    transforms.RandomPerspective(),\n","    transform_no_aug,\n","])\n","transform_fifth_aug = transforms.Compose([\n","    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","    transform_no_aug,\n","])\n","transform_combined_aug = transforms.Compose([\n","    transforms.RandomCrop(32, padding=8),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.RandomPerspective(),\n","    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","    transform_no_aug,\n","])\n","transform_aug_list = [transform_no_aug, transform_first_aug, transform_second_aug, transform_third_aug, transform_fourth_aug, transform_fifth_aug, transform_combined_aug]\n","aug_total = 7"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9ah9FfzCxpM","colab_type":"text"},"source":["# Import CIFAR-10"]},{"cell_type":"code","metadata":{"id":"bNnPALiBC2DH","colab_type":"code","outputId":"f3bf5424-9828-4173-f621-9323073b23a9","executionInfo":{"status":"ok","timestamp":1577571914317,"user_tz":300,"elapsed":10083,"user":{"displayName":"Shirley Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBVYN2BNsGw3UtqaoWorwLwNSlYLWOkTgMs5s-Bw=s64","userId":"16946191163708183763"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_no_aug)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","num_classes = 10\n","classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["170500096it [00:06, 26614380.71it/s]                               \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mHe7_xQlCiLx","colab_type":"text"},"source":["# Create balanced subsets"]},{"cell_type":"code","metadata":{"id":"eQy3LsQo59px","colab_type":"code","colab":{}},"source":["# Make each class in subsets balanced\n","num_subset = 10\n","class_split = [[], [], [], [], [], [], [], [], [], []]\n","for i in range(len(trainset)):\n","    img = trainset.data[i]\n","    to_tensor = transforms.ToTensor()\n","    img = to_tensor(img)\n","    class_split[trainset.targets[i]].append([img, trainset.targets[i]])\n","\n","# Store dataloader for each subset (10% - 100%) into a list\n","subset_list = []\n","for k in range(0, num_subset):\n","    crt_subset = []\n","    for i in range(0, num_classes):\n","        random.shuffle(class_split[i])\n","        for j in range(0, int(len(trainloader.dataset)*0.1*(k+1)*0.1)):\n","            crt_subset.append(class_split[i][j])\n","    random.shuffle(crt_subset)\n","    subset_list.append(torch.utils.data.DataLoader(crt_subset, batch_size=128, shuffle=True, num_workers=2))"],"execution_count":0,"outputs":[]}]}